import pandas as pd
import xgboost as xgb
import joblib
import os
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import accuracy_score, classification_report

# --- CONFIG ---
DATA_PATH = "backend/ds_service/data/synthetic_diabetes_data.csv"
MODEL_PATH = "backend/ds_service/models/food_safety_model.pkl"

def train():
    print("ğŸš€ Starting Model Training...")

    # load the training data generated by generate_synthetic_data.py
    if not os.path.exists(DATA_PATH):
        print(f"âŒ Error: Data file not found at {DATA_PATH}")
        print("Run 'python -m backend.ds_service.data.generate_synthetic_data' first.")
        return

    df = pd.read_csv(DATA_PATH)

    # X = the 9 input features (glucose readings, food nutrition, etc.)
    # y = the label we want to predict (1 = safe to eat right now, 0 = not safe)
    X = df.drop(columns=['is_safe'])
    y = df['is_safe']

    print(f"   Loaded {len(df)} rows.")
    print(f"   Features: {list(X.columns)}")

    # 80/20 split â€” the model trains on 80% and gets tested on the 20% it never saw.
    # random_state=42 just makes the split reproducible across runs.
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # XGBoost classifier â€” gradient boosted trees, works really well on tabular data.
    # n_estimators=100: builds 100 decision trees sequentially, each correcting the last.
    # max_depth=5: limits how deep each tree can go (prevents overfitting).
    # learning_rate=0.1: how much each tree contributes to the final answer.
    model = xgb.XGBClassifier(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=5,
        eval_metric='logloss'
    )

    # cross-validation: trains the model 5 times on different chunks of the training data
    # to check if accuracy is consistent. if one fold scores 99% and another scores 70%,
    # something is wrong with the data distribution.
    # StratifiedKFold makes sure each fold has the same safe/unsafe class ratio.
    print("\nğŸ”„ Running 5-Fold Cross-Validation...")
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')

    print(f"   CV Scores per fold: {cv_scores}")
    print(f"   âœ… Average CV Accuracy: {cv_scores.mean():.2%} (+/- {cv_scores.std() * 2:.2%})")

    if cv_scores.std() > 0.03:
        print("   âš ï¸ Warning: High variance in model performance. Data might be too noisy.")

    # now that we know the architecture is stable, train on the full training set
    print("\nğŸ’ª Training final model on full training set...")
    model.fit(X_train, y_train)

    # evaluate on the hold-out test set â€” these examples were never used during training
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"ğŸ“Š Final Test Set Accuracy: {accuracy:.2%}")
    print("\nDetailed Report:")
    print(classification_report(y_test, y_pred))

    # save the trained model to disk as a .pkl file.
    # the prediction service loads this at runtime with joblib.load().
    os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
    joblib.dump(model, MODEL_PATH)
    print(f"ğŸ’¾ Model saved to: {MODEL_PATH}")

    # feature importance â€” tells us which of the 9 inputs the model leaned on most.
    # glucose_level and food_gi usually dominate, which makes physiological sense.
    print("\nğŸ” Top 5 Most Important Features:")
    importance = model.feature_importances_
    feats = pd.DataFrame({'Feature': X.columns, 'Importance': importance})
    print(feats.sort_values(by='Importance', ascending=False).head(5))

if __name__ == "__main__":
    train()
